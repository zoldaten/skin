• какие модели использованы

модели по запросу

• как устроен pipeline

при запуске поднимается server на Flask, через форму подгружаем картинку, которая отправляется на локальный сервер с запущенной LLM моделью.
далее получаем результат в приложении, в том числе в JSON на диск.

• как запустить локально

python test4.py

Пример запроса/ответа (JSON).

Ответ приложен.

(Опционально) — короткое видео-демо 1–2 минуты.

https://youtu.be/Ka5dEnqtxRw
